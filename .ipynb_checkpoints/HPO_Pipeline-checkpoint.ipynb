{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.97,0.96,0.91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [0.94,0.95,0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score_with_std(score,k=1):\n",
    "    return np.mean(score) - k*np.std(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667\n",
      "0.026246692913372675\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(a))\n",
    "print(np.std(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667\n",
      "0.004714045207910321\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(b))\n",
    "print(np.std(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9419526214587564"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_with_std(b,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920419973753294"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_with_std(a,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_score_with_std(pipe):\n",
    "    cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    "    score = cross_val_score(pipe,X,y,cv=cv)\n",
    "    \n",
    "    #score = score_minius_k_std(score,k=1)  用来计算score = 平均值-K倍标准差 体现pipeline的稳定性\n",
    "    return score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_after_rs():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background : 各种score的意思"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. balanced_accury_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "y_true = [0, 1, 0, 1, 1, 0]\n",
    "y_pred = [0, 1, 0, 0, 0, 1]\n",
    "balanced_accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. average_precision_score\n",
    "只适用于binary classification task or multilabel classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "average_precision_score(y_true, y_scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. brier_score_loss\n",
    "The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false  \n",
    "也只适用于 binary classification or multilabel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris().data,load_iris().target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "RandomizedSearchCV took 0.60 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.543 (std: 0.027)\n",
      "Parameters: {'min_samples_split': 0.2, 'max_features': 5, 'max_depth': 3, 'criterion': 'gini', 'bootstrap': False}\n",
      "\n",
      "GridSearchCV took 2.97 seconds for 72 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.818 (std: 0.028)\n",
      "Parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits,load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# get some data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=2)\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=1):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "#param_dist = {\"max_depth\": [3, None],\n",
    "#              \"max_features\": sp_randint(1, 5),\n",
    "#              \"min_samples_split\": sp_randint(2, 5),\n",
    "#              \"bootstrap\": [True, False],\n",
    "#              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=5, iid=False)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_randfloat(a,b,n_iter_search=20):\n",
    "    return [random.uniform(a, b) for _ in range(n_iter_search)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_best_param(clf,param_dist):\n",
    "    search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=5, iid=False)\n",
    "    start = time()\n",
    "    search.fit(X,y)\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "    return search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 98.38 seconds for 20 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "cv_result = rs_best_param(ada,param_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.823 (std: 0.033)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 1.6201181415334593, 'n_estimators': 333}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'algorithm': 'SAMME', 'learning_rate': 1.6201181415334593, 'n_estimators': 479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "\n",
    "# np.random.uniform(5,10) # A single value\n",
    "param_dist = {'algorithm':['SAMME.R','SAMME'],\n",
    "              'n_estimators':sp_randint(50, 500),\n",
    "              'learning_rate':sp_randfloat(0.01,2)}\n",
    "\n",
    "ada_search = RandomizedSearchCV(ada, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=5, iid=False)\n",
    "ada_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(ada_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "ber = BernoulliNB()\n",
    "\n",
    "param_dist = {'alpha': [random.uniform(0.01, 2) for _ in range(n_iter_search)],\n",
    "             'fit_prior':[True,False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'criterion':['gini','entropy'],\n",
    "             'min_samples_split':sp_randint(2,20),\n",
    "             'min_samples_leaf':sp_randint(1,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "ext = ExtraTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'criterion':['gini','entropy'],\n",
    "             'max_feature':sp_randfloat(0.0,1.0),\n",
    "             'min_samples_split':sp_randint(2,20),\n",
    "             'min_samples_leaf':sp_randint(1,20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试pipeline中各个方法的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from Code.setup import *\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris().data,load_iris().target\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit,cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元素的排列组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_dataset(dataset_name):\n",
    "    print(\"Loading the dataset\")\n",
    "    df = pd.read_csv(dataset_name)\n",
    "    X = np.array(df.iloc[:,:-1])\n",
    "    y = np.array(df.iloc[:,-1])\n",
    "    return X,y\n",
    "X,y = load_dataset(\"Datasets/dataset_38_sick_le.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def test_func2(num_list):\n",
    "    res_list=[]\n",
    "    for i in range(len(num_list)+1):\n",
    "        res_list+=list(combinations(num_list, i))\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_tuple(tup):\n",
    "    ls = []\n",
    "    for i in tup:\n",
    "        ls.append((i,search_space_dic[i]()))\n",
    "    ls.append(('m3',search_space_dic['m3']()))\n",
    "    ls = Pipeline(ls)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_pipelines(search_space_dic):\n",
    "    search_space_names = list(search_space_dic.keys())\n",
    "    combi = test_func2(search_space_names[:6])[:7]\n",
    "    combi = map(change_to_tuple,combi)\n",
    "    combi_pipe = list(combi)\n",
    "    return combi_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def save_pipe_performance(combi_pipe):\n",
    "    df = pd.DataFrame(columns=['name','mean','std'])\n",
    "    for p in combi_pipe:\n",
    "        score = cross_val_score(p,X,y,cv=cv)\n",
    "        df = df.append({'name':list(p.named_steps.keys()),'mean':score.mean(),'std':score.std()},ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi_pipe = creat_pipelines(search_space_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_m1 = save_pipe_performance(combi_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[m3]</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>0.017778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[d1, m3]</td>\n",
       "      <td>0.928889</td>\n",
       "      <td>0.032660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[d2, m3]</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.019876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[d3, m3]</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.024343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[f1, m3]</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>0.017778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[f2, m3]</td>\n",
       "      <td>0.275556</td>\n",
       "      <td>0.022662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[f3, m3]</td>\n",
       "      <td>0.937778</td>\n",
       "      <td>0.043090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name      mean       std\n",
       "0      [m3]  0.964444  0.017778\n",
       "1  [d1, m3]  0.928889  0.032660\n",
       "2  [d2, m3]  0.955556  0.019876\n",
       "3  [d3, m3]  0.933333  0.024343\n",
       "4  [f1, m3]  0.964444  0.017778\n",
       "5  [f2, m3]  0.275556  0.022662\n",
       "6  [f3, m3]  0.937778  0.043090"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_better_methods(df):\n",
    "    base_score = df['mean'][0]\n",
    "    df_better = df[df['mean']>df['mean'][0]]\n",
    "    return df_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_better_features(df):\n",
    "    n_iter = df.shape[0]\n",
    "    features = [df.iloc[i,0][0] for i in range(n_iter)]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = find_better_methods(df_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = find_better_features(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA: 先单个搜索，和base比较，选择出表现更好的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Code.setup import *\n",
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris().data,load_iris().target\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_names = list(candidate_methods.keys())[:6]\n",
    "model_names = list(candidate_methods.keys())[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_and_feature_model(names,model):\n",
    "    feature_pipe = []\n",
    "    base_pipe = Pipeline([(model,candidate_methods[model])])\n",
    "    for name in names:\n",
    "        feature_pipe.append(Pipeline([(name,candidate_methods[name]),(model,candidate_methods[model])]))\n",
    "    return base_pipe,feature_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipe,feature_pipe = get_base_and_feature_model(preprocessor_names,model_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_better_features(feature_pipe,base):\n",
    "    base_score = cross_val_score(base,X,y,cv=cv)\n",
    "    print(\"BASE score:\",round(base_score.mean(),3))\n",
    "    features = []\n",
    "    for p in feature_pipe:\n",
    "        score = cross_val_score(p,X,y,cv=cv)\n",
    "        print(round(score.mean(),3))\n",
    "        if score.mean()>base_score.mean():\n",
    "            features.append(list(p.named_steps.keys())[0])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE score: 0.947\n",
      "0.942\n",
      "0.942\n",
      "0.942\n",
      "0.924\n",
      "0.973\n",
      "0.938\n"
     ]
    }
   ],
   "source": [
    "F = find_better_features(feature_pipe,base_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FastICA']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于这几个来讲FastICA表现更好，创建组合, 调整HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best pipeline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "beter_pipe = Pipeline([('FastICS',FastICA()),('Ada',AdaBoostClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(beter_pipe,X,y,cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9422222222222223"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After HOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA()\n",
    "X_transed = ica.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_randint(a,b,n_iter_search=20):\n",
    "    return [i for i in range(a,b+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_randfloat(a,b,n_iter_search=20):\n",
    "    return [random.uniform(a, b) for _ in range(n_iter_search)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'algorithm':['SAMME.R','SAMME'],\n",
    "              'n_estimators':sp_randint(50, 500),\n",
    "              'learning_rate':sp_randfloat(0.01,2)}\n",
    "\n",
    "#ada_search = RandomizedSearchCV(ada, param_distributions=param_dist,\n",
    "#                                   n_iter=n_iter_search, cv=5, iid=False)\n",
    "#ada_search.fit(X, y)\n",
    "#print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "#      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "#report(ada_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_search = RandomizedSearchCV(ada, param_distributions=param_dist,\n",
    "                                   n_iter=100, cv=5, iid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "          fit_params=None, iid=False, n_iter=100, n_jobs=None,\n",
       "          param_distributions={'algorithm': ['SAMME.R', 'SAMME'], 'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a239869e8>, 'learning_rate': [0.8008658588029912, 0.5336544800560077, 0.4825714564534939, 0.6712983329237802, 0.04553708177071152, 0.8028664688577358, 0.6959111414931873,... 0.29085184689155463, 1.7778051785328024, 1.698657707036429, 1.488824241405287, 0.5604893024891223]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_search.fit(X_transed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 1.7778051785328024, 'n_estimators': 445}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 0.5604893024891223, 'n_estimators': 276}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 1.7778051785328024, 'n_estimators': 383}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 1.7778051785328024, 'n_estimators': 368}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 0.04553708177071152, 'n_estimators': 326}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 1.860703179185936, 'n_estimators': 275}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 0.26740799863060644, 'n_estimators': 155}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 0.04553708177071152, 'n_estimators': 466}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 0.04553708177071152, 'n_estimators': 485}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 1.860703179185936, 'n_estimators': 367}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 1.860703179185936, 'n_estimators': 335}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 0.04553708177071152, 'n_estimators': 275}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 1.0968256032303838, 'n_estimators': 357}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 1.860703179185936, 'n_estimators': 375}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.980 (std: 0.027)\n",
      "Parameters: {'algorithm': 'SAMME', 'learning_rate': 1.0968256032303838, 'n_estimators': 409}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "report(ada_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
